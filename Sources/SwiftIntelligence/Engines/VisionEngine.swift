// VisionEngine.swift
// SwiftIntelligence - Advanced Computer Vision
// Copyright Â© 2024 Muhittin Camdali. MIT License.

import Foundation
import Vision
import CoreML
import CoreImage
#if canImport(UIKit)
import UIKit
#elseif canImport(AppKit)
import AppKit
#endif

/// High-performance computer vision engine with on-device AI
@available(iOS 17.0, macOS 14.0, tvOS 17.0, watchOS 10.0, visionOS 1.0, *)
public actor VisionEngine {
    
    // MARK: - Singleton
    
    public static let shared = VisionEngine()
    
    // MARK: - Properties
    
    private let ciContext = CIContext(options: [.useSoftwareRenderer: false])
    private var classificationModel: VNCoreMLModel?
    private var objectDetectionModel: VNCoreMLModel?
    private var cache = NSCache<NSString, AnyObject>()
    
    // MARK: - Initialization
    
    private init() {
        cache.countLimit = 50
        cache.totalCostLimit = 50_000_000
        
        Task {
            await loadModels()
        }
    }
    
    private func loadModels() async {
        // Load pre-trained models
        // In production, these would load actual Core ML models
    }
    
    // MARK: - Image Classification
    
    /// Classify image contents using state-of-the-art AI
    public func classify(_ image: SIImage) async throws -> ClassificationResult {
        let startTime = Date()
        
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        // Check cache
        let cacheKey = NSString(string: "classify_\(image.hashValue)")
        if let cached = cache.object(forKey: cacheKey) as? ClassificationResult {
            return cached
        }
        
        // Use Vision framework for classification
        let request = VNClassifyImageRequest()
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        try handler.perform([request])
        
        guard let observations = request.results else {
            throw VisionError.classificationFailed
        }
        
        let labels = observations.prefix(10).map { observation in
            ClassificationResult.LabelScore(
                label: observation.identifier.capitalized,
                confidence: observation.confidence
            )
        }
        
        let result = ClassificationResult(
            labels: Array(labels),
            processingTime: Date().timeIntervalSince(startTime)
        )
        
        // Cache result
        cache.setObject(result as AnyObject, forKey: cacheKey)
        
        return result
    }
    
    // MARK: - Object Detection
    
    /// Detect objects in an image with bounding boxes
    public func detectObjects(
        in image: SIImage,
        maxObjects: Int = 20
    ) async throws -> [DetectedObject] {
        
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)
        
        // Use Vision for object detection
        let request = VNRecognizeAnimalsRequest()
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        try handler.perform([request])
        
        var detectedObjects: [DetectedObject] = []
        
        if let results = request.results {
            for observation in results.prefix(maxObjects) {
                let boundingBox = convertBoundingBox(observation.boundingBox, imageSize: imageSize)
                
                for label in observation.labels {
                    let object = DetectedObject(
                        label: label.identifier.capitalized,
                        confidence: label.confidence,
                        boundingBox: boundingBox,
                        category: "animal"
                    )
                    detectedObjects.append(object)
                }
            }
        }
        
        // Also try rectangle detection for general objects
        let rectangleRequest = VNDetectRectanglesRequest()
        rectangleRequest.maximumObservations = maxObjects
        
        try handler.perform([rectangleRequest])
        
        if let rectangles = rectangleRequest.results {
            for observation in rectangles {
                let boundingBox = convertBoundingBox(observation.boundingBox, imageSize: imageSize)
                let object = DetectedObject(
                    label: "Object",
                    confidence: observation.confidence,
                    boundingBox: boundingBox,
                    category: "object"
                )
                detectedObjects.append(object)
            }
        }
        
        return detectedObjects
    }
    
    // MARK: - Face Detection
    
    /// Detect faces in an image with landmarks
    public func detectFaces(in image: SIImage) async throws -> [Face] {
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)
        
        // Face detection with landmarks
        let faceRequest = VNDetectFaceLandmarksRequest()
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        try handler.perform([faceRequest])
        
        guard let results = faceRequest.results else {
            return []
        }
        
        return results.map { observation in
            let boundingBox = convertBoundingBox(observation.boundingBox, imageSize: imageSize)
            
            var landmarks: FaceLandmarks?
            if let visionLandmarks = observation.landmarks {
                landmarks = FaceLandmarks(
                    leftEye: visionLandmarks.leftEye?.normalizedPoints.first.map {
                        CGPoint(x: $0.x * imageSize.width, y: (1 - $0.y) * imageSize.height)
                    } ?? .zero,
                    rightEye: visionLandmarks.rightEye?.normalizedPoints.first.map {
                        CGPoint(x: $0.x * imageSize.width, y: (1 - $0.y) * imageSize.height)
                    } ?? .zero,
                    nose: visionLandmarks.nose?.normalizedPoints.first.map {
                        CGPoint(x: $0.x * imageSize.width, y: (1 - $0.y) * imageSize.height)
                    } ?? .zero,
                    mouth: visionLandmarks.outerLips?.normalizedPoints.first.map {
                        CGPoint(x: $0.x * imageSize.width, y: (1 - $0.y) * imageSize.height)
                    } ?? .zero,
                    jawline: visionLandmarks.faceContour?.normalizedPoints.map {
                        CGPoint(x: $0.x * imageSize.width, y: (1 - $0.y) * imageSize.height)
                    } ?? []
                )
            }
            
            return Face(
                boundingBox: boundingBox,
                confidence: observation.confidence,
                landmarks: landmarks,
                age: nil,
                emotion: nil
            )
        }
    }
    
    // MARK: - Text Recognition (OCR)
    
    /// Extract text from an image
    public func extractText(
        from image: SIImage,
        languages: [String]
    ) async throws -> TextExtractionResult {
        
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)
        
        let request = VNRecognizeTextRequest()
        request.recognitionLevel = .accurate
        request.recognitionLanguages = languages
        request.usesLanguageCorrection = true
        
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        try handler.perform([request])
        
        guard let results = request.results else {
            return TextExtractionResult(text: "", blocks: [], confidence: 0)
        }
        
        var fullText = ""
        var blocks: [TextExtractionResult.TextBlock] = []
        var totalConfidence: Float = 0
        
        for observation in results {
            guard let candidate = observation.topCandidates(1).first else { continue }
            
            let boundingBox = convertBoundingBox(observation.boundingBox, imageSize: imageSize)
            
            fullText += candidate.string + "\n"
            totalConfidence += candidate.confidence
            
            blocks.append(TextExtractionResult.TextBlock(
                text: candidate.string,
                boundingBox: boundingBox,
                confidence: candidate.confidence
            ))
        }
        
        let avgConfidence = results.isEmpty ? 0 : totalConfidence / Float(results.count)
        
        return TextExtractionResult(
            text: fullText.trimmingCharacters(in: .whitespacesAndNewlines),
            blocks: blocks,
            confidence: avgConfidence
        )
    }
    
    // MARK: - Image Description
    
    /// Generate natural language description of an image
    public func describe(_ image: SIImage) async throws -> String {
        // Combine classification and object detection for description
        let classification = try await classify(image)
        let objects = try await detectObjects(in: image, maxObjects: 5)
        let faces = try await detectFaces(in: image)
        
        var descriptions: [String] = []
        
        // Add main classification
        if let topLabel = classification.topLabel {
            descriptions.append("This image appears to show \(topLabel.lowercased())")
        }
        
        // Add detected objects
        if !objects.isEmpty {
            let objectLabels = objects.prefix(3).map { $0.label.lowercased() }
            let objectString = objectLabels.joined(separator: ", ")
            descriptions.append("containing \(objectString)")
        }
        
        // Add face info
        if !faces.isEmpty {
            let faceCount = faces.count
            descriptions.append("with \(faceCount) \(faceCount == 1 ? "person" : "people") visible")
        }
        
        return descriptions.joined(separator: " ")
    }
    
    // MARK: - Image Segmentation
    
    /// Segment image into meaningful regions
    public func segment(_ image: SIImage) async throws -> SegmentationResult {
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        // Use person segmentation as primary segmentation
        let request = VNGeneratePersonSegmentationRequest()
        request.qualityLevel = .accurate
        
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        try handler.perform([request])
        
        guard let result = request.results?.first else {
            return SegmentationResult(segments: [], maskImage: nil)
        }
        
        // Convert mask to image
        let maskCIImage = CIImage(cvPixelBuffer: result.pixelBuffer)
        guard let maskCGImage = ciContext.createCGImage(maskCIImage, from: maskCIImage.extent) else {
            throw VisionError.segmentationFailed
        }
        
        let segment = SegmentationResult.Segment(
            label: "person",
            confidence: 0.9,
            mask: Data()
        )
        
        return SegmentationResult(
            segments: [segment],
            maskImage: SIImage(cgImage: maskCGImage)
        )
    }
    
    // MARK: - Background Removal
    
    /// Remove background from image
    public func removeBackground(from image: SIImage) async throws -> SIImage {
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        // Generate person segmentation mask
        let request = VNGeneratePersonSegmentationRequest()
        request.qualityLevel = .accurate
        request.outputPixelFormat = kCVPixelFormatType_OneComponent8
        
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        try handler.perform([request])
        
        guard let result = request.results?.first else {
            throw VisionError.segmentationFailed
        }
        
        // Apply mask to original image
        let originalCIImage = CIImage(cgImage: cgImage)
        let maskCIImage = CIImage(cvPixelBuffer: result.pixelBuffer)
        
        // Scale mask to match original
        let scaleX = originalCIImage.extent.width / maskCIImage.extent.width
        let scaleY = originalCIImage.extent.height / maskCIImage.extent.height
        let scaledMask = maskCIImage.transformed(by: CGAffineTransform(scaleX: scaleX, y: scaleY))
        
        // Blend with transparent background
        guard let blendFilter = CIFilter(name: "CIBlendWithMask") else {
            throw VisionError.processingFailed
        }
        
        blendFilter.setValue(originalCIImage, forKey: kCIInputImageKey)
        blendFilter.setValue(CIImage.empty(), forKey: kCIInputBackgroundImageKey)
        blendFilter.setValue(scaledMask, forKey: kCIInputMaskImageKey)
        
        guard let outputImage = blendFilter.outputImage,
              let outputCGImage = ciContext.createCGImage(outputImage, from: outputImage.extent) else {
            throw VisionError.processingFailed
        }
        
        return SIImage(cgImage: outputCGImage)
    }
    
    // MARK: - Image Enhancement
    
    /// Enhance image quality with AI upscaling
    public func enhance(_ image: SIImage, scale: Float) async throws -> SIImage {
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        let ciImage = CIImage(cgImage: cgImage)
        
        // Calculate new size
        let newWidth = ciImage.extent.width * CGFloat(scale)
        let newHeight = ciImage.extent.height * CGFloat(scale)
        
        // Use Lanczos scaling for high-quality upscale
        guard let lanczosFilter = CIFilter(name: "CILanczosScaleTransform") else {
            throw VisionError.processingFailed
        }
        
        lanczosFilter.setValue(ciImage, forKey: kCIInputImageKey)
        lanczosFilter.setValue(scale, forKey: kCIInputScaleKey)
        lanczosFilter.setValue(1.0, forKey: kCIInputAspectRatioKey)
        
        guard let scaledImage = lanczosFilter.outputImage else {
            throw VisionError.processingFailed
        }
        
        // Apply sharpening
        let sharpenedImage = scaledImage.applyingFilter("CIUnsharpMask", parameters: [
            "inputRadius": 2.0,
            "inputIntensity": 0.5
        ])
        
        // Apply slight vibrance boost
        let enhancedImage = sharpenedImage.applyingFilter("CIVibrance", parameters: [
            "inputAmount": 0.2
        ])
        
        guard let outputCGImage = ciContext.createCGImage(enhancedImage, from: enhancedImage.extent) else {
            throw VisionError.processingFailed
        }
        
        return SIImage(cgImage: outputCGImage)
    }
    
    // MARK: - Reset
    
    public func reset() async {
        cache.removeAllObjects()
    }
    
    // MARK: - Helpers
    
    private func convertBoundingBox(_ normalizedBox: CGRect, imageSize: CGSize) -> CGRect {
        let x = normalizedBox.origin.x * imageSize.width
        let y = (1 - normalizedBox.origin.y - normalizedBox.height) * imageSize.height
        let width = normalizedBox.width * imageSize.width
        let height = normalizedBox.height * imageSize.height
        
        return CGRect(x: x, y: y, width: width, height: height)
    }
}

// MARK: - SIImage Extension

#if canImport(UIKit)
extension UIImage {
    var cgImage: CGImage? {
        return self.cgImage
    }
}
#elseif canImport(AppKit)
extension NSImage {
    var cgImage: CGImage? {
        return self.cgImage(forProposedRect: nil, context: nil, hints: nil)
    }
}
#endif

// MARK: - Vision Errors

public enum VisionError: LocalizedError {
    case invalidImage
    case classificationFailed
    case detectionFailed
    case segmentationFailed
    case processingFailed
    case modelNotAvailable
    
    public var errorDescription: String? {
        switch self {
        case .invalidImage: return "Invalid or corrupted image"
        case .classificationFailed: return "Image classification failed"
        case .detectionFailed: return "Object detection failed"
        case .segmentationFailed: return "Image segmentation failed"
        case .processingFailed: return "Image processing failed"
        case .modelNotAvailable: return "Required model not available"
        }
    }
}
